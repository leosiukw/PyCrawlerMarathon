{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT 網路爬蟲實作練習\n",
    "\n",
    "\n",
    "* 能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "根據範例 ，完成以下問題：\n",
    "\n",
    "* ① 印出最新文章的「作者」「標題」「時間」\n",
    "* ② 印出第一頁所有文章的「作者」「標題」「時間」\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 印出最新文章的「作者」「標題」「時間」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bainite (黑羽毛 ┐(′д`)┌) [情報] Joakim Noah和快艇簽新約 Sat Jun 20 23:17:55 2020\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ptt.cc/bbs/NBA/index.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html5lib\")\n",
    "\n",
    "d1 = soup.find(class_=\"title\")\n",
    "url1 = 'https://www.ptt.cc' + d1.find('a')['href']\n",
    "\n",
    "r1 = requests.get(url1)\n",
    "soup1 = BeautifulSoup(r1.text, \"html5lib\")\n",
    "\n",
    "author = soup1.find_all(class_=\"article-meta-value\")[0].text\n",
    "title = soup1.find_all(class_=\"article-meta-value\")[2].text\n",
    "time = soup1.find_all(class_=\"article-meta-value\")[3].text\n",
    "\n",
    "print(author, title, time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② 印出第一頁所有文章的「作者」「標題」「時間」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bainite (黑羽毛 ┐(′д`)┌) [情報] Joakim Noah和快艇簽新約 Sat Jun 20 23:17:55 2020\n",
      "delete70923 (House) [討論] 七年前的今天，熱火完成二連霸 Sun Jun 21 08:30:03 2020\n",
      "greprep (@@) [花邊] 歐肥談到當年魔術續約時的心情轉變 Sun Jun 21 10:27:28 2020\n",
      "pneumo (超☆冒險蓋) Re: [花邊] NBA》想約凡妮莎先問過我拳頭！科比硬漢 Sun Jun 21 11:18:49 2020\n",
      "avig800 (養貓也有政治正確?) Re: [情報] 球員同盟聲明:我們已經受夠天賦被人利用 Sun Jun 21 12:05:46 2020\n",
      "SeungYeon (我愛勝妍) Re: [花邊] 歐肥談到當年魔術續約時的心情轉變 Sun Jun 21 12:25:59 2020\n",
      "ckny (辛苦的歲月) [情報] Butler被小孩叫「尼Ｘ」:他的父親教他仇恨 Sun Jun 21 13:10:18 2020\n",
      "ckny (辛苦的歲月) [花邊] Mitchell、Trae Young的訓練照 Sun Jun 21 14:24:26 2020\n",
      "asdf1256 (guest) [新聞] 因禍得福？柯爾：下季將看到最佳版本的 Sun Jun 21 18:04:49 2020\n",
      "bbbyes123 (冬日將至) [討論] NBA有哪些球員很愛翹班嗎 Sun Jun 21 20:46:53 2020\n",
      "thnlkj0665 (灰色地帶) [花邊] 評防守喬丹布萊恩詹皇 亞泰斯特:喬丹最有效 Sun Jun 21 20:55:21 2020\n",
      "thnlkj0665 (灰色地帶) [情報] JaVale McGee 練投三分 Sun Jun 21 22:40:00 2020\n",
      "ckny (辛苦的歲月) [專欄] Kanter想灌籃卻扭到腰 被Fall虧爆(影片) Sun Jun 21 22:52:21 2020\n",
      "willy911006 (小溫) [情報] Anthony Tolliver和灰熊達成口頭協議 Mon Jun 22 18:30:43 2020\n",
      "asdf1256 (guest) [新聞] 震撼教育時刻？勇士菜鳥：被詹皇霸凌 Mon Jun 22 22:04:43 2020\n",
      "thnlkj0665 (灰色地帶) [情報] Mike Brown 面試尼克隊總教練 Mon Jun 22 22:16:30 2020\n",
      "ak658865 () [討論] 打超過十年，待超過十隊的球員？ Mon Jun 22 22:26:16 2020\n",
      "ckny (辛苦的歲月) [專欄] Kerr:差點因KT賽末出手發火，發現是Curry Mon Jun 22 22:37:27 2020\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-2f5d33a9a335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mr_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_single\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msoup_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html5lib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url_list=[]\n",
    "\n",
    "for d in soup.find_all(class_=\"title\"):\n",
    "    url_list.append('https://www.ptt.cc' + d.find('a')['href'])\n",
    "\n",
    "for url_single in url_list[:-1]:\n",
    "    r_single = requests.get(url_single)\n",
    "    soup_single = BeautifulSoup(r_single.text, \"html5lib\")\n",
    "    author = soup_single.find_all(class_=\"article-meta-value\")[0].text\n",
    "    title = soup_single.find_all(class_=\"article-meta-value\")[2].text\n",
    "    time = soup_single.find_all(class_=\"article-meta-value\")[3].text\n",
    "    print(author, title, time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ 試著爬爬看其他版的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mOoOm ( Simon) [新聞] 質疑蘋果、Google APP商店壟斷！微軟總 Mon Jun 22 21:26:25 2020\n",
      "eyespot (追求內心的自在) [公告] ncc5566 水桶一個月 Mon Jun 22 21:27:07 2020\n",
      "cokia80913 (多久沒吃了) Re: [心得] 保留PTT的尊嚴好嗎？ Mon Jun 22 21:29:36 2020\n",
      "S9125126 (SS) [標的] 緯創 多 Mon Jun 22 21:36:25 2020\n",
      "eyespot (追求內心的自在) [公告] longfish95 shencookie 警告一次 Mon Jun 22 21:41:31 2020\n",
      "blades (回甘就像現泡) Re: [標的] 6633 萊鎂醫 多 Mon Jun 22 21:46:48 2020\n",
      "eyespot (追求內心的自在) [公告] Taiwanese TDK56 警告一次 Mon Jun 22 21:46:54 2020\n",
      "eyespot (追求內心的自在) [公告] menShow 水桶一週 Mon Jun 22 21:56:39 2020\n",
      "ZMittermeyer (ZM) Re: [標的] Zoom 空 Mon Jun 22 22:03:43 2020\n",
      "star720 (超級喜歡) [標的] 6269 台郡 無腦壓力空沒輸過 Mon Jun 22 22:03:48 2020\n",
      "hrma (資深象迷) [標的] (多)黃金 Mon Jun 22 22:12:11 2020\n",
      "yjjia (天橋底下說書人) Re: [標的] 4414 如興 Mon Jun 22 22:19:04 2020\n",
      "Su22 (裝配匠) [新聞] 海隆控股稱無法按期償還票據 今年中資美 Mon Jun 22 22:22:53 2020\n",
      "cjol (勤樸) [新聞] 中印衝突引發排中潮！印度網友抵制小米 Mon Jun 22 22:27:12 2020\n",
      "aidssm1234 (久逃) [新聞]台微體 搶海外商機 Mon Jun 22 22:32:30 2020\n",
      "Su22 (裝配匠) [新聞] 前台積電老將、武漢弘芯CEO蔣尚義傳退意  Mon Jun 22 22:34:06 2020\n",
      "CavendishJr (伊井野梅西子) [新聞] Nokia法國子公司將裁員1233人　約占1/3  Mon Jun 22 22:36:10 2020\n",
      "j547 (j547) [標的] AMD 多 Mon Jun 22 22:46:40 2020\n",
      "vovorr (旅行放鬆) [請益] 10萬元投資選擇權,慘被老爸罵 Mon Jun 22 23:09:06 2020\n",
      "Su22 (裝配匠) [新聞] 剔除華為 加拿大Telus與三星合作5G Mon Jun 22 23:29:56 2020\n",
      "IanLi (IanLi) [公告] 精華區導覽Q&A Sun Jan 25 23:18:20 2015\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-4c2ef74ad171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mr_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_single\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msoup_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html5lib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_single\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-meta-value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url_stock = 'https://www.ptt.cc/bbs/Stock/index.html'\n",
    "r_stock = requests.get(url_stock)\n",
    "\n",
    "soup_stock = BeautifulSoup(r_stock.text, \"html5lib\")\n",
    "\n",
    "url_list=[]\n",
    "\n",
    "for d in soup_stock.find_all(class_=\"title\"):\n",
    "    url_list.append('https://www.ptt.cc' + d.find('a')['href'])\n",
    "\n",
    "for url_single in url_list[:-1]:\n",
    "    r_single = requests.get(url_single)\n",
    "    soup_single = BeautifulSoup(r_single.text, \"html5lib\")\n",
    "    author = soup_single.find_all(class_=\"article-meta-value\")[0].text\n",
    "    title = soup_single.find_all(class_=\"article-meta-value\")[2].text\n",
    "    time = soup_single.find_all(class_=\"article-meta-value\")[3].text\n",
    "    print(author, title, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
